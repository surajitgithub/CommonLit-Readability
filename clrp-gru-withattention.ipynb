{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada8066f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-30T21:53:13.698226Z",
     "iopub.status.busy": "2021-07-30T21:53:13.697009Z",
     "iopub.status.idle": "2021-07-30T21:53:13.709526Z",
     "shell.execute_reply": "2021-07-30T21:53:13.710028Z",
     "shell.execute_reply.started": "2021-07-30T21:27:24.212197Z"
    },
    "papermill": {
     "duration": 0.037935,
     "end_time": "2021-07-30T21:53:13.710297",
     "exception": false,
     "start_time": "2021-07-30T21:53:13.672362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n",
      "/kaggle/input/commonlitreadabilityprize/train.csv\n",
      "/kaggle/input/commonlitreadabilityprize/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c41153",
   "metadata": {
    "papermill": {
     "duration": 0.020229,
     "end_time": "2021-07-30T21:53:13.751962",
     "exception": false,
     "start_time": "2021-07-30T21:53:13.731733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Look at the Features\n",
    "https://www.kaggle.com/donmarch14/commonlit-detailed-guide-to-learn-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44734c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:53:13.802044Z",
     "iopub.status.busy": "2021-07-30T21:53:13.800976Z",
     "iopub.status.idle": "2021-07-30T21:54:01.325364Z",
     "shell.execute_reply": "2021-07-30T21:54:01.325880Z",
     "shell.execute_reply.started": "2021-07-30T21:27:24.223809Z"
    },
    "papermill": {
     "duration": 47.553895,
     "end_time": "2021-07-30T21:54:01.326049",
     "exception": false,
     "start_time": "2021-07-30T21:53:13.772154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge, LinearRegression, LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams    \n",
    "\n",
    "import html\n",
    "import unicodedata\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da65a617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:01.371303Z",
     "iopub.status.busy": "2021-07-30T21:54:01.370688Z",
     "iopub.status.idle": "2021-07-30T21:54:01.380821Z",
     "shell.execute_reply": "2021-07-30T21:54:01.381324Z",
     "shell.execute_reply.started": "2021-07-30T21:27:33.739402Z"
    },
    "papermill": {
     "duration": 0.033826,
     "end_time": "2021-07-30T21:54:01.381504",
     "exception": false,
     "start_time": "2021-07-30T21:54:01.347678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920cf3ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:01.426166Z",
     "iopub.status.busy": "2021-07-30T21:54:01.425517Z",
     "iopub.status.idle": "2021-07-30T21:54:01.445414Z",
     "shell.execute_reply": "2021-07-30T21:54:01.445997Z",
     "shell.execute_reply.started": "2021-07-30T21:27:33.754080Z"
    },
    "papermill": {
     "duration": 0.043832,
     "end_time": "2021-07-30T21:54:01.446192",
     "exception": false,
     "start_time": "2021-07-30T21:54:01.402360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wordcloud(text,ngram=1):\n",
    "    wordcloud = WordCloud(width=1400, \n",
    "                            height=800,\n",
    "                            random_state=2021,\n",
    "                            background_color='black',\n",
    "                            )\n",
    "    if ngram ==1:\n",
    "        wordc = wordcloud.generate(' '.join(text))\n",
    "    else:\n",
    "        wordc = wordcloud.generate_from_frequencies(text)\n",
    "    plt.figure(figsize=(12,6), facecolor='k')\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout(pad=0)\n",
    "      \n",
    "\n",
    "def get_n_grans_count(text, n_grams, min_freq):\n",
    "    output = {}\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    #Create the n_gram\n",
    "    if n_grams == 2:\n",
    "        gs = nltk.bigrams(tokens)\n",
    "        \n",
    "    elif n_grams == 3:\n",
    "        gs = nltk.trigrams(tokens)\n",
    "\n",
    "    else:\n",
    "        return 'Only 2_grams and 3_grams are supported'\n",
    "    \n",
    "    # compute frequency distribution for all the bigrams in the text by threshold with min_freq\n",
    "    fdist = nltk.FreqDist(gs)\n",
    "    for k,v in fdist.items():\n",
    "        if v > min_freq:\n",
    "            index = ' '.join(k)\n",
    "            output[index] = v\n",
    "    \n",
    "    return output\n",
    "    \n",
    "def remove_special_chars(text):\n",
    "    re1 = re.compile(r'  +')\n",
    "    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n",
    "        ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x1))\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def remove_stopwords(words, stop_words):\n",
    "    \"\"\"\n",
    "    :param words:\n",
    "    :type words:\n",
    "    :param stop_words: from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "    or\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    :type stop_words:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in text\"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize words in text, and by defult lemmatize nouns\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in text\"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word, pos='v') for word in words])\n",
    "\n",
    "def text2words(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def normalize_text( text):\n",
    "    text = remove_special_chars(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = replace_numbers(text)\n",
    "    words = text2words(text)\n",
    "    words = remove_stopwords(words, stop_words)\n",
    "    #words = stem_words(words)# Either stem or lemmatize\n",
    "    words = lemmatize_words(words)\n",
    "    words = lemmatize_verbs(words)\n",
    "\n",
    "    return ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c101f883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:01.491868Z",
     "iopub.status.busy": "2021-07-30T21:54:01.491236Z",
     "iopub.status.idle": "2021-07-30T21:54:11.588716Z",
     "shell.execute_reply": "2021-07-30T21:54:11.588030Z",
     "shell.execute_reply.started": "2021-07-30T21:27:33.780119Z"
    },
    "papermill": {
     "duration": 10.121394,
     "end_time": "2021-07-30T21:54:11.588869",
     "exception": false,
     "start_time": "2021-07-30T21:54:01.467475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_raw_train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "df_raw_train.head()\n",
    "input_train, input_val, target_train, target_val = train_test_split(df_raw_train['excerpt'], df_raw_train['target'], test_size=0.2)\n",
    "df_raw_train['excerpt'] = [normalize_text(sent) for sent in df_raw_train['excerpt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36de2a96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:11.642040Z",
     "iopub.status.busy": "2021-07-30T21:54:11.641072Z",
     "iopub.status.idle": "2021-07-30T21:54:11.645183Z",
     "shell.execute_reply": "2021-07-30T21:54:11.645720Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.000427Z"
    },
    "papermill": {
     "duration": 0.035738,
     "end_time": "2021-07-30T21:54:11.645907",
     "exception": false,
     "start_time": "2021-07-30T21:54:11.610169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2573   -0.962189\n",
       "185    -0.290306\n",
       "127    -2.462000\n",
       "831    -0.634706\n",
       "33     -1.053898\n",
       "          ...   \n",
       "1512   -1.369069\n",
       "2193   -1.199020\n",
       "355    -0.849320\n",
       "2487   -1.494803\n",
       "1490   -1.286254\n",
       "Name: target, Length: 567, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "885e4ba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:11.698710Z",
     "iopub.status.busy": "2021-07-30T21:54:11.698083Z",
     "iopub.status.idle": "2021-07-30T21:54:11.700306Z",
     "shell.execute_reply": "2021-07-30T21:54:11.700801Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.013459Z"
    },
    "papermill": {
     "duration": 0.031526,
     "end_time": "2021-07-30T21:54:11.700967",
     "exception": false,
     "start_time": "2021-07-30T21:54:11.669441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_raw_train[['excerpt','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f68047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:11.746889Z",
     "iopub.status.busy": "2021-07-30T21:54:11.746264Z",
     "iopub.status.idle": "2021-07-30T21:54:11.753065Z",
     "shell.execute_reply": "2021-07-30T21:54:11.753501Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.029560Z"
    },
    "papermill": {
     "duration": 0.031159,
     "end_time": "2021-07-30T21:54:11.753689",
     "exception": false,
     "start_time": "2021-07-30T21:54:11.722530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<oov>')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset():\n",
    "    #targ_val, inp_lang = df_train['target'],df_train['excerpt']\n",
    "    targ_val = df_train['target']\n",
    "    inp_lang = get_lang()\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    return input_tensor, inp_lang_tokenizer\n",
    "\n",
    "def get_lang():\n",
    "    en=[]\n",
    "    for i in df_train['excerpt']:\n",
    "        en_1 = [w for w in i.split(' ')]\n",
    "        en.append(en_1)\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef409838",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:11.799213Z",
     "iopub.status.busy": "2021-07-30T21:54:11.798593Z",
     "iopub.status.idle": "2021-07-30T21:54:12.244364Z",
     "shell.execute_reply": "2021-07-30T21:54:12.244894Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.044112Z"
    },
    "papermill": {
     "duration": 0.470079,
     "end_time": "2021-07-30T21:54:12.245083",
     "exception": false,
     "start_time": "2021-07-30T21:54:11.775004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor, inp_lang = load_dataset()\n",
    "target_tensor = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddfeab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.291435Z",
     "iopub.status.busy": "2021-07-30T21:54:12.290797Z",
     "iopub.status.idle": "2021-07-30T21:54:12.298981Z",
     "shell.execute_reply": "2021-07-30T21:54:12.299948Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.531588Z"
    },
    "papermill": {
     "duration": 0.033694,
     "end_time": "2021-07-30T21:54:12.300262",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.266568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2267 2267 567 567\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90e4428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.371857Z",
     "iopub.status.busy": "2021-07-30T21:54:12.371013Z",
     "iopub.status.idle": "2021-07-30T21:54:12.392315Z",
     "shell.execute_reply": "2021-07-30T21:54:12.392820Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.542404Z"
    },
    "papermill": {
     "duration": 0.069913,
     "end_time": "2021-07-30T21:54:12.393008",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.323095",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "tar_int_size = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b981aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.449126Z",
     "iopub.status.busy": "2021-07-30T21:54:12.448401Z",
     "iopub.status.idle": "2021-07-30T21:54:12.471853Z",
     "shell.execute_reply": "2021-07-30T21:54:12.471237Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.593593Z"
    },
    "papermill": {
     "duration": 0.056882,
     "end_time": "2021-07-30T21:54:12.472015",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.415133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44dc34df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.518695Z",
     "iopub.status.busy": "2021-07-30T21:54:12.518062Z",
     "iopub.status.idle": "2021-07-30T21:54:12.535824Z",
     "shell.execute_reply": "2021-07-30T21:54:12.535295Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.629090Z"
    },
    "papermill": {
     "duration": 0.041978,
     "end_time": "2021-07-30T21:54:12.535976",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.493998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_int_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.fc = tf.keras.layers.Dense(tar_int_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        self.fc2 = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "#         print(x.shape)\n",
    "#         output, state = self.gru(x)\n",
    "#         output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(x)\n",
    "        x = self.fc2(x)\n",
    "        return x, attention_weights\n",
    "\n",
    "decoder = Decoder(tar_int_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23a519e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.585708Z",
     "iopub.status.busy": "2021-07-30T21:54:12.585084Z",
     "iopub.status.idle": "2021-07-30T21:54:12.588408Z",
     "shell.execute_reply": "2021-07-30T21:54:12.587904Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.651593Z"
    },
    "papermill": {
     "duration": 0.030581,
     "end_time": "2021-07-30T21:54:12.588563",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.557982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    print(real,pred)\n",
    "    \n",
    "    loss_  = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa13580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.638972Z",
     "iopub.status.busy": "2021-07-30T21:54:12.638310Z",
     "iopub.status.idle": "2021-07-30T21:54:12.641092Z",
     "shell.execute_reply": "2021-07-30T21:54:12.641527Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.660529Z"
    },
    "papermill": {
     "duration": 0.030195,
     "end_time": "2021-07-30T21:54:12.641709",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.611514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcee29fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.692670Z",
     "iopub.status.busy": "2021-07-30T21:54:12.692026Z",
     "iopub.status.idle": "2021-07-30T21:54:12.696485Z",
     "shell.execute_reply": "2021-07-30T21:54:12.695958Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.674035Z"
    },
    "papermill": {
     "duration": 0.033217,
     "end_time": "2021-07-30T21:54:12.696655",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.663438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "#         print(dec_hidden.shape)\n",
    "        #dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        dec_input = enc_hidden\n",
    "        # Teacher forcing\n",
    "        #for t in range(1, targ.shape[1]):\n",
    "        prediction,attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "        loss = loss_function(targ, prediction)\n",
    "        #dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))      \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "716f2d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.748739Z",
     "iopub.status.busy": "2021-07-30T21:54:12.747740Z",
     "iopub.status.idle": "2021-07-30T21:54:12.750220Z",
     "shell.execute_reply": "2021-07-30T21:54:12.750685Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.686286Z"
    },
    "papermill": {
     "duration": 0.032358,
     "end_time": "2021-07-30T21:54:12.750853",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.718495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    EPOCHS = epochs\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start = time.time()\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                             batch,\n",
    "                                                             batch_loss.numpy()))\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                          total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "        \n",
    "# train_model(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5ca8c77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.805250Z",
     "iopub.status.busy": "2021-07-30T21:54:12.804225Z",
     "iopub.status.idle": "2021-07-30T21:54:12.808802Z",
     "shell.execute_reply": "2021-07-30T21:54:12.808160Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.703622Z"
    },
    "papermill": {
     "duration": 0.036063,
     "end_time": "2021-07-30T21:54:12.808943",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.772880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fc9ce4fc510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "max_length_inp =  max_length(input_tensor)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8352a9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.867905Z",
     "iopub.status.busy": "2021-07-30T21:54:12.867146Z",
     "iopub.status.idle": "2021-07-30T21:54:12.870120Z",
     "shell.execute_reply": "2021-07-30T21:54:12.869562Z",
     "shell.execute_reply.started": "2021-07-30T21:48:25.167243Z"
    },
    "papermill": {
     "duration": 0.039071,
     "end_time": "2021-07-30T21:54:12.870260",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.831189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = []\n",
    "for key in inp_lang.word_index.items():\n",
    "    words.append(key[0])\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = normalize_text(sentence)\n",
    "    inputs = [inp_lang.word_index[i] if i in words else 1 for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = enc_hidden\n",
    "    pred, attention_weights = decoder(dec_input,dec_hidden,enc_out)\n",
    "#     print(pred.shape)\n",
    "    return pred.numpy()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1592dfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:12.920800Z",
     "iopub.status.busy": "2021-07-30T21:54:12.920047Z",
     "iopub.status.idle": "2021-07-30T21:54:13.239572Z",
     "shell.execute_reply": "2021-07-30T21:54:13.239089Z",
     "shell.execute_reply.started": "2021-07-30T21:27:44.733301Z"
    },
    "papermill": {
     "duration": 0.346831,
     "end_time": "2021-07-30T21:54:13.239722",
     "exception": false,
     "start_time": "2021-07-30T21:54:12.892891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.068189025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(u'politicians do not have permission to do what needs to be done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "287b0762",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:54:13.298886Z",
     "iopub.status.busy": "2021-07-30T21:54:13.292937Z",
     "iopub.status.idle": "2021-07-30T21:55:21.296825Z",
     "shell.execute_reply": "2021-07-30T21:55:21.297332Z",
     "shell.execute_reply.started": "2021-07-30T21:27:45.051516Z"
    },
    "papermill": {
     "duration": 68.035253,
     "end_time": "2021-07-30T21:55:21.297498",
     "exception": false,
     "start_time": "2021-07-30T21:54:13.262245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE 1.4823420227120443\n"
     ]
    }
   ],
   "source": [
    "validation_df = pd.DataFrame(input_val,columns=[\"excerpt\"])\n",
    "validation_df['target']= target_val\n",
    "validation_df['prediction'] = validation_df['excerpt'].apply(lambda value : evaluate(value))\n",
    "validation_df['deviation'] = validation_df['target'] - validation_df['prediction']\n",
    "validation_df.sort_values('deviation',ascending =False)\n",
    "print('RMSE',np.sqrt(np.sum(np.square(validation_df['deviation']))/validation_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04ec5d54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-30T21:55:21.349089Z",
     "iopub.status.busy": "2021-07-30T21:55:21.348401Z",
     "iopub.status.idle": "2021-07-30T21:55:22.224092Z",
     "shell.execute_reply": "2021-07-30T21:55:22.224600Z",
     "shell.execute_reply.started": "2021-07-30T21:48:54.434108Z"
    },
    "papermill": {
     "duration": 0.904707,
     "end_time": "2021-07-30T21:55:22.224803",
     "exception": false,
     "start_time": "2021-07-30T21:55:21.320096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>0.061745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>0.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>0.061636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>0.062164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>0.064557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>0.065979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.060114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661  0.061745\n",
       "1  f0953f0a5  0.061224\n",
       "2  0df072751  0.061636\n",
       "3  04caf4e0c  0.062164\n",
       "4  0e63f8bea  0.064557\n",
       "5  12537fe78  0.065979\n",
       "6  965e592c0  0.060114"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "submission = pd.read_csv(\"../input/commonlitreadabilityprize/sample_submission.csv\")\n",
    "test_data['predict'] = test_data['excerpt'].apply(lambda value : evaluate(value))\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = test_data['id']\n",
    "submission['target'] = test_data['predict']\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fcd24",
   "metadata": {
    "papermill": {
     "duration": 0.023253,
     "end_time": "2021-07-30T21:55:22.271432",
     "exception": false,
     "start_time": "2021-07-30T21:55:22.248179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 138.849027,
   "end_time": "2021-07-30T21:55:24.364453",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-30T21:53:05.515426",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
